{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "instance_type": "ml.t3.medium",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcosoares-92/IndustrialDataScience_DataExtraction/blob/main/1_Data_Extraction_IP21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Extraction from Aspen InfoPlus21 (IP21)**"
      ],
      "metadata": {
        "azdata_cell_guid": "56e88df2-940c-4c28-aed4-129d8f9624e3",
        "id": "ICDehudjTYN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _Data Extraction Workflow Notebook 1_"
      ],
      "metadata": {
        "azdata_cell_guid": "5de346f0-9c91-4407-a651-c3e29cd3c122",
        "id": "JEM8k4TUTYOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content:\n",
        "1. Retrieving a list of dataframes returned from each API call;\n",
        "2. Fetching or updating a SQLite database;\n",
        "3. Mounting the storage system (S3 or Google Drive);\n",
        "4. Loading previously obtained Pandas dataframe;\n",
        "5. Concatenating (SQL UNION) multiple dataframes;\n",
        "6. Removing duplicate rows from the dataframe;\n",
        "7. Exporting the dataframe as CSV File (to notebook's workspace);\n",
        "8. Importing or exporting models, lists, or dictionaries;\n",
        "9. Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory;\n",
        "10. Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)."
      ],
      "metadata": {
        "azdata_cell_guid": "b76328b1-fd4d-4d2d-a04a-db2bcec1990d",
        "id": "TcRs07tATYO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "\n",
        "IP21 API runs only in Firefox and Chrome. If Edge or other browser is being used, run the following command in a cell:\n",
        "\n",
        "- `!jupyter notebook list`\n",
        "\n",
        "- or run `jupyter notebook list` in the Anaconda command line.\n",
        "\n",
        "This will show an URL like:\n",
        "    \n",
        "`http://localhost:8888/?token=TOKEN_VALUE :: C:\\Users\\Dir`\n",
        "\n",
        "- Then, open your Firefox, Chrome, or other browser, access the page http://localhost:8888, and copy and paste the token represented by `TOKEN_VALUE` in the correspondent input box."
      ],
      "metadata": {
        "azdata_cell_guid": "e35f00fe-9539-4c1f-880c-45222fcb6795",
        "id": "khGgKj95TYO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
        "- marcosoares.feq@gmail.com\n",
        "- marco.soares@bayer.com"
      ],
      "metadata": {
        "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267",
        "id": "yFCRWWz2TYPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Python Libraries in Global Context**"
      ],
      "metadata": {
        "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea",
        "id": "av6oTL86TYPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import load\n",
        "from idsw import *"
      ],
      "metadata": {
        "azdata_cell_guid": "69207e17-a485-420d-a8af-415e9686c30c",
        "language": "python",
        "id": "v4-pI1QjTYPH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Call the functions**"
      ],
      "metadata": {
        "azdata_cell_guid": "5aebe18e-c82f-48c4-8e93-9c735c5ad6b1",
        "id": "zHUhoX1XyHHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Retrieving a list of dataframes returned from each API call**"
      ],
      "metadata": {
        "azdata_cell_guid": "fd30a174-feb6-4807-9283-1015576a9878",
        "id": "wdwQMCqzTYPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IP21_SERVER = 'ip21_server_name'\n",
        "# IP21_SERVER is a string informing the server name for the IP21 REST API.\n",
        "# If you check ASPEN ONE or ASPEN IP21 REST API URL, it will have a format like:\n",
        "# http://ip21_server_name/ProcessData/AtProcessDataREST.dll/\n",
        "# or like:\n",
        "# http://ip21_server_name.company_website/processexplorer/aspenONE.html\n",
        "# In this case, declare:\n",
        "# IP21_SERVER = 'ip21_server_name' or as 'ip21_server_name/'\n",
        "\n",
        "LIST_OF_TAGS_TO_EXTRACT = [{'tag': None, 'actual_name': None}]\n",
        "# LIST_OF_TAGS_TO_EXTRACT = [{'tag': None, 'actual_name': None}] is a list of dictionaries.\n",
        "# The dictionaries should have always the same keys: 'tag', containing the tag name as registered\n",
        "# in the system, and 'actual_name', with a desired name for the variable. You can add as much\n",
        "# tags as you want, but adding several tags may lead to a blockage by the server. The key 'actual_name'\n",
        "# may be empty, but dictionaries where the 'tag' value is None will be ignored.\n",
        "# Examples: LIST_OF_TAGS_TO_EXTRACT = [{'tag': 'TEMP', 'actual_name': 'temperature'}]\n",
        "# LIST_OF_TAGS_TO_EXTRACT = [{'tag': 'TEMP2.1.2', 'actual_name': 'temperature'},\n",
        "# {'tag': 'PUMP.1.2', 'actual_name': 'pump_pressure'}, {'tag': 'PHTANK', 'actual_name': 'ph'}]\n",
        "# LIST_OF_TAGS_TO_EXTRACT = [{'tag': 'TEMP', 'actual_name': None}]\n",
        "\n",
        "USERNAME = None\n",
        "PASSWORD = None\n",
        "# USERNAME = None, PASSWORD = None: declare your username and password as strings (in quotes)\n",
        "# or keep username = None, password = None to generate input boxes. The key typed on the boxes\n",
        "# will be masked, so other users cannot see it.\n",
        "\n",
        "DATA_SOURCE = 'localhost'\n",
        "# DATA_SOURCE = 'localhost': string informing the particular data source to fetch on IP21.\n",
        "# Keep DATA_SOURCE = 'localhost' to query all available data sources.\n",
        "\n",
        "START_TIME = {'year': 2015, 'month': 1, 'day':1, 'hour': 0, 'minute': 0, 'second': 0}\n",
        "STOP_TIME = {'year': 2022, 'month': 4, 'day': 1, 'hour': 0, 'minute': 0, 'second': 0}\n",
        "# START_TIME: dictionary containing start timestamp information.\n",
        "# Example: START_TIME = {'year': 2015, 'month': 1, 'day':1, 'hour': 0, 'minute': 0, 'second': 0}\n",
        "# STOP_TIME: dictionary containing stop timestamp information.\n",
        "# Example: STOP_TIME = {'year': 2022, 'month': 4, 'day': 1, 'hour': 0, 'minute': 0, 'second': 0}\n",
        "\n",
        "# Alternatively: START_TIME = 'today', 'now', START_TIME = 'yesterday', START_TIME = -10 for 10\n",
        "# days before, START_TIME = -X for - X days before. Units for offsets will be always in days, unless\n",
        "# you modify the parameters START_TIMEDELTA_UNIT and STOP_TIMEDELTA_UNIT.\n",
        "# For the timedelta unit, set 'day' or 'd' for subtracting values in days,'hour' or 'h',\n",
        "# 'minute' or 'm' for minutes, 'second' or 's' for seconds, 'milisecond' or 'ms' for miliseconds.\n",
        "# Put the \"-\" signal, or the time will be interpreted as a future day from today.\n",
        "# Analogously for stop_time.\n",
        "# Both dictionaries must contain only float values (for 'year', 'day' and 'month' are integers,\n",
        "# naturally).\n",
        "\n",
        "## WARNING: The keys must be always be the same, only change the numeric values.\n",
        "## The keys must be: 'year', 'month', 'day', 'hour', 'minute', and 'second'\n",
        "\n",
        "START_TIMEDELTA_UNIT = 'day'\n",
        "STOP_TIMEDELTA_UNIT = 'day'\n",
        "# START_TIMEDELTA_UNIT = 'day'\n",
        "# If START_TIME was declared as a numeric value (integer or float), specify the timescale units\n",
        "# in this parameter. The possible values are: 'day' or 'd'; 'hour' or 'h'; 'minute' or 'm';\n",
        "# 'second' or 's', 'milisecond' or 'ms'.\n",
        "# STOP_TIMEDELTA_UNIT = 'day' - analogous to START_TIMEDELTA_UNIT. Set this parameter when\n",
        "# declaring STOP_TIME as a numeric value.\n",
        "\n",
        "IP21TIME_ARRAY = []\n",
        "# IP21TIME_ARRAY = [] - keep this parameter as an empty list or set IP21TIME_ARRAY = None.\n",
        "# If you want to use the method to independently convert an array, you could pass this array\n",
        "# to the constructor to convert it.\n",
        "\n",
        "PREVIOUS_DATAFRAME_FOR_CONCATENATION = None\n",
        "# PREVIOUS_DATAFRAME_FOR_CONCATENATION = None: keep it None or, if you want to append the fetched data\n",
        "# to a pre-existing database, declare the object containing the pandas dataframe where it will\n",
        "# be appended. Example: PREVIOUS_DATAFRAME_FOR_CONCATENATION = dataset.\n",
        "\n",
        "\n",
        "# list of dataframes returned from each API call returned as returned_dfs_list.\n",
        "# Simply modify this object on the left of the equality\n",
        "returned_dfs_list = get_data_from_ip21 (ip21_server = IP21_SERVER, list_of_tags_to_extract = LIST_OF_TAGS_TO_EXTRACT, username = USERNAME, password = PASSWORD, data_source = DATA_SOURCE, start_time = START_TIME, stop_time = STOP_TIME, start_timedelta_unit = START_TIMEDELTA_UNIT, stop_timedelta_unit = STOP_TIMEDELTA_UNIT, ip21time_array = IP21TIME_ARRAY, previous_df_for_concatenation = PREVIOUS_DATAFRAME_FOR_CONCATENATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "7b5707ef-b2cf-40ef-9745-98ade531e156",
        "language": "python",
        "id": "TJGI5wGhTYPR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fetching or updating a SQLite database**"
      ],
      "metadata": {
        "azdata_cell_guid": "fd30a174-feb6-4807-9283-1015576a9878",
        "id": "2cFji50ITYPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = '/my_db.db'\n",
        "# FILE_PATH: full path of the SQLite file. It may start with './' or '/',\n",
        "# but with no more than 2 slashes.\n",
        "# It is a string: input in quotes. Example: FILE_PATH = '/my_db.db'\n",
        "\n",
        "TABLE_NAME = 'main_table'\n",
        "# TABLE_NAME: string with the name of the table that will be fetched or updated.\n",
        "# Example: TABLE_NAME = 'main_table'\n",
        "\n",
        "ACTION = 'fetch_table'\n",
        "# ACTION = 'fetch_table' to access a table named TABLE_NAME from the database.\n",
        "# ACTION = 'update_table' to update a table named TABLE_NAME from the database.\n",
        "\n",
        "PRE_CREATED_ENGINE = None\n",
        "# PRE_CREATED_ENGINE = None - if None, a new engine will be created. If an engine was already created,\n",
        "# pass it as argument: PRE_CREATED_ENGINE = engine\n",
        "\n",
        "DATASET = None\n",
        "# DATASET = None - if a table is going to be updated, input here the new Pandas dataframe\n",
        "# (object) correspondent to the table.\n",
        "# Example: DATASET = dataset.\n",
        "\n",
        "\n",
        "# Dataframe correspondent to the fetched (or updated) table returned as sqlite_df.\n",
        "# Engine used for manipulating the database returned as engine.\n",
        "# Simply modify these objects in the left of the equality:\n",
        "sqlite_df, engine = manipulate_sqlite_db (file_path = FILE_PATH, table_name = TABLE_NAME, action = ACTION, pre_created_engine = PRE_CREATED_ENGINE, df = DATASET)"
      ],
      "metadata": {
        "azdata_cell_guid": "8feb99d8-1cfd-433a-81aa-2cda31cd6459",
        "language": "python",
        "id": "cGmcl0kUTYPT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
      ],
      "metadata": {
        "azdata_cell_guid": "d5b101c0-0d41-4e63-9def-a883eeec85e8",
        "id": "M8u2xmAbTYPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .csv (comma separated values)\n",
        "\n",
        "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
        "# Alternatively: object containing the dataset to be exported.\n",
        "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
        "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\"\n",
        "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
        "\n",
        "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "9a01d46a-381f-4d89-99a4-050cf977b1b4",
        "language": "python",
        "id": "AmiCqi20TYPb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting dataframes as Excel file tables**"
      ],
      "metadata": {
        "azdata_cell_guid": "d8ec5628-69b5-455f-8fde-080fc9694b93",
        "id": "rDOGfVHWTYPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .xlsx\n",
        "\n",
        "FILE_NAME_WITHOUT_EXTENSION = \"datasets\"\n",
        "# (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. new_file_name_without_extension = \"my_file\"\n",
        "# will export a file 'my_file.xlsx' to notebook's workspace.\n",
        "\n",
        "EXPORTED_TABLES = [{'dataframe_obj_to_be_exported': None,\n",
        "                    'excel_sheet_name': None},]\n",
        "\n",
        "# exported_tables is a list of dictionaries. User may declare several dictionaries,\n",
        "# as long as the keys are always the same, and if the values stored in keys are not None.\n",
        "\n",
        "# key 'dataframe_obj_to_be_exported': dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "# key 'excel_sheet_name': string containing the name of the sheet to be written on the\n",
        "# exported Excel file. Example: excel_sheet_name = 'tab_1' will save the dataframe in the\n",
        "# sheet 'tab_1' from the file named as file_name_without_extension.\n",
        "\n",
        "# examples: exported_tables = [{'dataframe_obj_to_be_exported': dataset1,\n",
        "# 'excel_sheet_name': 'sheet1'},]\n",
        "# will export only dataset1 as 'sheet1';\n",
        "# exported_tables = [{'dataframe_obj_to_be_exported': dataset1, 'excel_sheet_name': 'sheet1'},\n",
        "# {'dataframe_obj_to_be_exported': dataset2, 'excel_sheet_name': 'sheet2']\n",
        "# will export dataset1 as 'sheet1' and dataset2 as 'sheet2'.\n",
        "\n",
        "# Notice that if the file does not contain the exported sheets, they will be created. If it has,\n",
        "# the sheets will be replaced.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "\n",
        "export_pd_dataframe_as_excel (file_name_without_extension = FILE_NAME_WITHOUT_EXTENSION, exported_tables = EXPORTED_TABLES, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "f6369c67-1585-4db1-844d-14484baa0f93",
        "language": "python",
        "id": "3X5a1IbYTYPi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1",
        "id": "CF_QNVcMTYPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scope in Azure Active Directory (AD) - What is a scope in azure security - Background**\n",
        "- AD: Active Directory allows network administrators to create and manage domains, users, and objects within a network. For example, an admin can create a group of users and give them specific access privileges to certain directories on the server.\n",
        "- The AD is the database that contains the security information and permissions of the users.\n",
        "- By limiting the scope, you limit what resources are at risk if the security principal is ever compromised. In Azure, you can specify a scope at four levels: management group, subscription, resource group, and resource. Scopes are structured in a parent-child relationship.\n",
        "#### Therefore, the Azure scope represents the security privileges of an user."
      ],
      "metadata": {
        "azdata_cell_guid": "b7a5708d-cbc9-4b85-b8bc-9534579b4f08",
        "id": "tjmNb6ERTYPq"
      }
    }
  ]
}